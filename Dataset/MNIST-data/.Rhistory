for(i in 1:num.images) {
l <- readBin(to.read, integer(), size=1, n=1, endian="big")
r <- as.integer(readBin(to.read, raw(), size=1, n=1024,
endian="big"))
g <- as.integer(readBin(to.read, raw(), size=1, n=1024,
endian="big"))
b <- as.integer(readBin(to.read, raw(), size=1, n=1024,
endian="big"))
index <- num.images * (f-1) + i
images.rgb[[index]] = data.frame(r, g, b)
images.lab[[index]] = l+1
}
close(to.read)
cat("completed :", filenames[f], "\n")
remove(l,r,g,b,f,i,index, to.read)
}
return(list("images.rgb"=images.rgb,"images.lab"=images.lab))
}
cifar_train <- read.cifar.data(filenames =
c("data_batch_1.bin","data_batch_2.bin","data_batch_3.bin","data_ba
tch_4.bin", "data_batch_5.bin"))
cifar_train <- read.cifar.data(filenames =
c("data_batch_1.bin","data_batch_2.bin","data_batch_3.bin","data_ba
tch_4.bin", "data_batch_5.bin"),num.images = 5)
cifar_train <- read.cifar.data(filenames =
c("data_batch_1.bin","data_batch_2.bin","data_batch_3.bin","data_batch_4.bin", "data_batch_5.bin"),num.images = 5)
images.rgb.train <- cifar_train$images.rgb
images.lab.train <- cifar_train$images.lab
rm(cifar_train)
cifar_test <- read.cifar.data(filenames = c("test_batch.bin"))
images.rgb.test <- cifar_test$images.rgb
images.lab.test <- cifar_test$images.lab
rm(cifar_test)
cifar_test <- read.cifar.data(filenames = c("test_batch.bin"), num.images = 1)
images.rgb.test <- cifar_test$images.rgb
images.lab.test <- cifar_test$images.lab
rm(cifar_test)
flat_data <- function(x_listdata,y_listdata){
x_listdata <- lapply(x_listdata,function(x){unlist(x)})
x_listdata <- do.call(rbind,x_listdata)
y_listdata <- lapply(y_listdata,function(x){a=c(rep(0,10)); a[x]=1;
return(a)})
y_listdata <- do.call(rbind,y_listdata)
return(list("images"=x_listdata, "labels"=y_listdata))
}
train_data <- flat_data(x_listdata = images.rgb.train, y_listdata =
images.lab.train)
test_data <- flat_data(x_listdata = images.rgb.test, y_listdata =
images.lab.test)
labels <- read.table("Cifar_10/batches.meta.txt")
drawImage <- function(index, images.rgb, images.lab=NULL) {
require(imager)
img <- images.rgb[[index]]
img.r.mat <- as.cimg(matrix(img$r, ncol=32, byrow = FALSE))
img.g.mat <- as.cimg(matrix(img$g, ncol=32, byrow = FALSE))
img.b.mat <- as.cimg(matrix(img$b, ncol=32, byrow = FALSE))
img.col.mat <- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")
if(!is.null(images.lab)){
lab = labels[[1]][images.lab[[index]]]
}
plot(img.col.mat,main=paste0(lab,":32x32 size",sep=" "),xaxt="n")
axis(side=1, xaxp=c(10, 50, 4), las=1)
return(list("Image label" =lab,"Image description" =img.col.mat))
}
drawImage(sample(1:50000, size=1), images.rgb.train,
images.lab.train)
img.col.mat <- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")
if(!is.null(images.lab)){
lab = labels[[1]][images.lab[[index]]]
}
plot(img.col.mat,main=paste0(lab,":32x32 size",sep=" "),xaxt="n")
axis(side=1, xaxp=c(10, 50, 4), las=1)
return(list("Image label" =lab,"Image description" =img.col.mat))
}
drawImage(sample(1:500, size=1), images.rgb.train,
images.lab.train)
img.col.mat <- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")
if(!is.null(images.lab)){
lab = labels[[1]][images.lab[[index]]]
}
plot(img.col.mat,main=paste0(lab,":32x32 size",sep=" "),xaxt="n")
axis(side=1, xaxp=c(10, 50, 4), las=1)
return(list("Image label" =lab,"Image description" =img.col.mat))
}
drawImage(sample(1:50, size=1), images.rgb.train,
images.lab.train)
drawImage <- function(index, images.rgb, images.lab=NULL) {
require(imager)
img <- images.rgb[[index]]
img.r.mat <- as.cimg(matrix(img$r, ncol=32, byrow = FALSE))
img.g.mat <- as.cimg(matrix(img$g, ncol=32, byrow = FALSE))
img.b.mat <- as.cimg(matrix(img$b, ncol=32, byrow = FALSE))
img.col.mat <- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")
if(!is.null(images.lab)){
lab = labels[[1]][images.lab[[index]]]
}
plot(img.col.mat,main=paste0(lab,":32x32 size",sep=" "),xaxt="n")
axis(side=1, xaxp=c(10, 50, 4), las=1)
return(list("Image label" =lab,"Image description" =img.col.mat))
}
drawImage(sample(1:50000, size=1), images.rgb.train,
images.lab.train)
Require(caret)
normalizeObj<-preProcess(train_data$images, method="range")
train_data$images<-predict(normalizeObj, train_data$images)
test_data$images <- predict(normalizeObj, test_data$images)
require(caret)
normalizeObj<-preProcess(train_data$images, method="range")
train_data$images<-predict(normalizeObj, train_data$images)
test_data$images <- predict(normalizeObj, test_data$images)
library(tensorflow)
sess = tf$Session()
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
library(text2vec)
library(glmnet)
data("movie_review")
logistic_model <- function(Xtrain,Ytrain,Xtest,Ytest)
{
classifier <- cv.glmnet(x=Xtrain, y=Ytrain,
family="binomial", alpha=1, type.measure = "auc",
nfolds = 5, maxit = 1000)
plot(classifier)
vocab_test_pred <- predict(classifier, Xtest, type = "response")
return(cat("Train AUC : ", round(max(classifier$cvm), 4),
"Test AUC : ",glmnet:::auc(Ytest, vocab_test_pred),"\n"))
}
train_samples <-
caret::createDataPartition(c(1:length(labels[1,1])),p =
0.8)$Resample1
label[1,1]
labels[1,1]
labels[,1]
length(labels[1,])
train_samples <-
caret::createDataPartition(c(1:length(labels[,1])),p =
0.8)$Resample1
library(h2o)
require(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O =
TRUE,min_mem_size = "20G",nthreads = 8)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O =
TRUE,min_mem_size = "20G",nthreads = 8)
localH2O = h2o.init(ip = "localhost", port = 54321, nthreads = -1)
load_occupancy_data<-function(train){
xFeatures = c("Temperature", "Humidity", "Light", "CO2",
"HumidityRatio")
yFeatures = "Occupancy"
if(train){
occupancy_ds <-
as.matrix(read.csv("datatraining.txt",stringsAsFactors = T))
} else
{
occupancy_ds <-
as.matrix(read.csv("datatest.txt",stringsAsFactors = T))
}
occupancy_ds<-apply(occupancy_ds[, c(xFeatures, yFeatures)], 2,
FUN=as.numeric)
return(occupancy_ds)
}
occupancy_train <-load_occupancy_data(train=T)
occupancy_test <- load_occupancy_data(train = F)
setwd("~/occupancy_data")
occupancy_train <-load_occupancy_data(train=T)
occupancy_test <- load_occupancy_data(train = F)
ggpairs(occupancy_train$data[, occupancy_train$xFeatures])
require(GGally)
ggpairs(occupancy_train$data[, occupancy_train$xFeatures])
occupancy_train$data<-data.frame(occupancy_train$data)
occupancy_train$data<-data.frame(occupancy_train$data)
setwd("~/")
occupancy_train$data<-data.frame(occupancy_train$data)
img_width = 32L
img_height = 32L
img_shape = c(img_width, img_height)
num_classes = 10L
num_channels = 3L
img_size_flat = img_width * img_height * num_channels
filter_size1 = 5L
num_filters1 = 64L
filter_size2 = 5L
num_filters2 = 64L
fc_size = 1024L
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
create_conv_layer <- function(input,
num_input_channels,
filter_size,
num_filters,
use_pooling=True)
{
shape1 = shape(filter_size, filter_size, num_input_channels,
num_filters)
weights = weight_variable(shape=shape1)
biases = bias_variable(shape=shape(num_filters))
layer = tf$nn$conv2d(input=input,
filter=weights,
strides=shape(1L, 1L, 1L ,1L),
padding="SAME")
layer = layer + biases
if(use_pooling){
layer = tf$nn$max_pool(value=layer,
ksize=shape(1L, 2L, 2L, 1L),
strides=shape(1L, 2L, 2L, 1L),
padding='SAME')
}
layer = tf$nn$relu(layer)
return(list("layer" = layer, "weights" = weights))
}
drawImage_conv <- function(index, images.bw,
images.lab=NULL,par_imgs=8) {
require(imager)
img <- images.bw[index,,,]
n_images <- dim(img)[3]
par(mfrow=c(par_imgs,par_imgs), oma=c(0,0,0,0),
mai=c(0.05,0.05,0.05,0.05),ann=FALSE,ask=FALSE)
for(i in 1:n_images){
img.bwmat <- as.cimg(img[,,i])
if(!is.null(images.lab)){
lab = labels[[1]][images.lab[[index]]]
}
plot(img.bwmat,axes=FALSE,ann=FALSE)
}
par(mfrow=c(1,1))
}
drawImage_conv_weights <- function(weights_conv, par_imgs=8) {
require(imager)
n_images <- dim(weights_conv)[4]
par(mfrow=c(par_imgs,par_imgs), oma=c(0,0,0,0),
mai=c(0.05,0.05,0.05,0.05),ann=FALSE,ask=FALSE)
for(i in 1:n_images){
img.r.mat <- as.cimg(weights_conv[,,1,i])
img.g.mat <- as.cimg(weights_conv[,,2,i])
img.b.mat <- as.cimg(weights_conv[,,3,i])
img.col.mat <- imappend(list(img.r.mat,img.g.mat,img.b.mat),"c")
plot(img.col.mat,axes=FALSE,ann=FALSE)
}
par(mfrow=c(1,1))
}
flatten_conv_layer <- function(layer){
layer_shape = layer$get_shape()
num_channels
num_features =
prod(c(layer_shape$as_list()[[2]],layer_shape$as_list()[[3]],layer_
shape$as_list()[[4]]))
layer_flat = tf$reshape(layer, shape(-1, num_features))
return(list("layer_flat"=layer_flat, "num_features"=num_features))
}
flatten_conv_layer <- function(layer){
layer_shape = layer$get_shape()
num_channels
num_features =
prod(c(layer_shape$as_list()[[2]],layer_shape$as_list()[[3]],layer_shape$as_list()[[4]]))
layer_flat = tf$reshape(layer, shape(-1, num_features))
return(list("layer_flat"=layer_flat, "num_features"=num_features))
}
create_fc_layer <- function(input,
num_inputs,
num_outputs,
use_relu=True)
{
weights = weight_variable(shape=shape(num_inputs, num_outputs))
biases = bias_variable(shape=shape(num_outputs))
layer = tf$matmul(input, weights) + biases
if(use_relu){
layer = tf$nn$relu(layer)
}
return(layer)
}
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
library(tensorflow)
sess = tf$Session()
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
x_image = tf$reshape(x, shape(-1L, img_size, img_size,
num_channels))
img_size = 32L
x_image = tf$reshape(x, shape(-1L, img_size, img_size,
num_channels))
y_true = tf$placeholder(tf$float32, shape=shape(NULL, num_classes),
name='y_true')
y_true_cls = tf$argmax(y_true, dimension=1L)
conv1 <- create_conv_layer(input=x_image,
num_input_channels=num_channels,
filter_size=filter_size1,
num_filters=num_filters1,
use_pooling=TRUE)
layer_conv1 <- conv1$layer
conv1_images <- conv1$layer$eval(feed_dict = dict(x = train_data$images, y_true = train_data$labels))
sess = tf$Session()
conv1_images <- conv1$layer$eval(feed_dict = dict(x = train_data$images, y_true = train_data$labels))
conv1_images <- conv1$layer$eval(feed_dict = dict(x = train_data$images, y_true = train_data$labels), session = sess)
install.packages("reticulate")
use_condaenv("python27")
midi <-
import_from_path("midi",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
np <- import("numpy")
msgpack <-
import_from_path("msgpack",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
psys <- import("sys")
tqdm <-
import_from_path("tqdm",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
midi_manipulation_updated <-
import_from_path("midi_manipulation_updated",path="C:/Music_RBM")
glob <- import("glob")
use_condaenv("python27")
library(reticulate)
use_condaenv("python27")
midi <-
import_from_path("midi",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
midi <-
import_from_path("midi",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
psys <- import("sys")
tqdm <-
import_from_path("tqdm",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
tqdm <-
import_from_path("tqdm",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
tqdm <-
import_from_path("tqdm",path="C:/ProgramData/Anaconda2/Lib/sitepackages")
require(tensorflow)
require(imager)
require(caret)
setwd("~/MNIST-data")
normalizeObj<-preProcess(trainData, method="range")
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot =
TRUE)
normalizeObj<-preProcess(trainData, method="range")
trainData<-t(apply(mnist$train$images, 1, FUN=reduceImage))
validData<-t(apply(mnist$test$images, 1, FUN=reduceImage))
reduceImage<-function(actds, n.pixel.x=16, n.pixel.y=16){
actImage<-matrix(actds, ncol=28, byrow=FALSE)
img.col.mat <- imappend(list(as.cimg(actImage)),"c")
thmb <- resize(img.col.mat, n.pixel.x, n.pixel.y)
outputImage<-matrix(thmb[,,1,1], nrow = 1, byrow = F)
return(outputImage)
}
trainData<-t(apply(mnist$train$images, 1, FUN=reduceImage))
validData<-t(apply(mnist$test$images, 1, FUN=reduceImage))
labels <- mnist$train$labels
labels_valid <- mnist$test$labels
normalizeObj<-preProcess(trainData, method="range")
trainData<-predict(normalizeObj, trainData)
validData<-predict(normalizeObj, validData)
library(tensorflow)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot =
TRUE)
reduceImage<-function(actds, n.pixel.x=16, n.pixel.y=16){
actImage<-matrix(actds, ncol=28, byrow=FALSE)
img.col.mat <- imappend(list(as.cimg(actImage)),"c")
thmb <- resize(img.col.mat, n.pixel.x, n.pixel.y)
outputImage<-matrix(thmb[,,1,1], nrow = 1, byrow = F)
return(outputImage)
}
trainData<-t(apply(mnist$train$images, 1, FUN=reduceImage))
validData<-t(apply(mnist$test$images, 1, FUN=reduceImage))
plot_mnist<-function(imageD, pixel.y=16){
actImage<-matrix(imageD, ncol=pixel.y, byrow=FALSE)
img.col.mat <- imappend(list(as.cimg(actImage)), "c")
plot(img.col.mat)
}
tf$reset_default_graph()
sess<-tf$InteractiveSession()
n_input=256
n.hidden.enc.1<-64
tf$reset_default_graph()
sess<-tf$InteractiveSession()
n_input=256
n.hidden.enc.1<-64
model_init<-function(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
{ weights<-NULL
weights[["encoder_w"]][["h1"]]=tf$Variable(xavier_init(n_input,n.hidden.enc.1))
weights[["encoder_w"]]
[["h2"]]=tf$Variable(xavier_init(n.hidden.enc.1, n.hidden.enc.2))
weights[["encoder_w"]][["out_mean"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))
weights[["encoder_w"]][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))
weights[["encoder_b"]][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.1), dtype=tf$float32))
weights[["encoder_b"]][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.2), dtype=tf$float32))
weights[["encoder_b"]][["out_mean"]]=tf$Variable(tf$zeros(shape(n_h
), dtype=tf$float32))
weights[["encoder_b"]][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))
weights[['decoder_w']][["h1"]]=tf$Variable(xavier_init(n_h,n.hidden.dec.1))
weights[['decoder_w']][["h2"]]=tf$Variable(xavier_init(n.hidden.dec.1, n.hidden.dec.2))
weights[['decoder_w']][["out_mean"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))
weights[['decoder_w']][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))
weights[['decoder_b']][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.1), dtype=tf$float32))
weights[['decoder_b']][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.2), dtype=tf$float32))
weights[['decoder_b']][["out_mean"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))
weights[['decoder_b']][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))
return(weights)
}
model_init<-function(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
{ weights<-NULL
weights[["encoder_w"]][["h1"]]=tf$Variable(xavier_init(n_input,n.hidden.enc.1))
weights[["encoder_w"]][["h2"]]=tf$Variable(xavier_init(n.hidden.enc.1, n.hidden.enc.2))
weights[["encoder_w"]][["out_mean"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))
weights[["encoder_w"]][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.enc.2, n_h))
weights[["encoder_b"]][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.1), dtype=tf$float32))
weights[["encoder_b"]][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.enc.2), dtype=tf$float32))
weights[["encoder_b"]][["out_mean"]]=tf$Variable(tf$zeros(shape(n_h
), dtype=tf$float32))
weights[["encoder_b"]][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_h), dtype=tf$float32))
weights[['decoder_w']][["h1"]]=tf$Variable(xavier_init(n_h,n.hidden.dec.1))
weights[['decoder_w']][["h2"]]=tf$Variable(xavier_init(n.hidden.dec.1, n.hidden.dec.2))
weights[['decoder_w']][["out_mean"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))
weights[['decoder_w']][["out_log_sigma"]]=tf$Variable(xavier_init(n.hidden.dec.2, n_input))
weights[['decoder_b']][["b1"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.1), dtype=tf$float32))
weights[['decoder_b']][["b2"]]=tf$Variable(tf$zeros(shape(n.hidden.dec.2), dtype=tf$float32))
weights[['decoder_b']][["out_mean"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))
weights[['decoder_b']][["out_log_sigma"]]=tf$Variable(tf$zeros(shape(n_input), dtype=tf$float32))
return(weights)
}
xavier_init<-function(n_inputs, n_outputs, constant=1){
low = -constant*sqrt(6.0/(n_inputs + n_outputs))
high = constant*sqrt(6.0/(n_inputs + n_outputs))
return(tf$random_uniform(shape(n_inputs, n_outputs), minval=low,
maxval=high, dtype=tf$float32))
}
vae_encoder<-function(x, weights, biases){
layer_1 = tf$nn$softplus(tf$add(tf$matmul(x, weights[['h1']]),
biases[['b1']]))
layer_2 = tf$nn$softplus(tf$add(tf$matmul(layer_1,
weights[['h2']]), biases[['b2']]))
z_mean = tf$add(tf$matmul(layer_2, weights[['out_mean']]),
biases[['out_mean']])
z_log_sigma_sq = tf$add(tf$matmul(layer_2,
weights[['out_log_sigma']]), biases[['out_log_sigma']])
return (list("z_mean"=z_mean, "z_log_sigma_sq"=z_log_sigma_sq))
}
vae_decoder<-function(z, weights, biases){
layer1<-tf$nn$softplus(tf$add(tf$matmul(z, weights[["h1"]]),
biases[["b1"]]))
layer2<-tf$nn$softplus(tf$add(tf$matmul(layer1, weights[["h2"]]),
biases[["b2"]]))
x_reconstr_mean<-tf$nn$sigmoid(tf$add(tf$matmul(layer2,
weights[['out_mean']]), biases[['out_mean']]))
return(x_reconstr_mean)
}
network_ParEval<-function(x, network_weights, n_h){
distParameter<-vae_encoder(x, network_weights[["encoder_w"]],
network_weights[["encoder_b"]])
z_mean<-distParameter$z_mean
z_log_sigma_sq <-distParameter$z_log_sigma_sq
eps = tf$random_normal(shape(BATCH, n_h), 0, 1, dtype=tf$float32)
z = tf$add(z_mean, tf$multiply(tf$sqrt(tf$exp(z_log_sigma_sq)),eps))
x_reconstr_mean <- vae_decoder(z, network_weights[["decoder_w"]],network_weights[["decoder_b"]])
return(list("x_reconstr_mean"=x_reconstr_mean,"z_log_sigma_sq"=z_log_sigma_sq, "z_mean"=z_mean))
}
vae_optimizer<-function(x, networkOutput){
x_reconstr_mean<-networkOutput$x_reconstr_mean
z_log_sigma_sq<-networkOutput$z_log_sigma_sq
z_mean<-networkOutput$z_mean
loss_reconstruction<--1*tf$reduce_sum(x*tf$log(1e-10 + x_reconstr_mean)+(1-x)*tf$log(1e-10 + 1 - x_reconstr_mean), reduction_indices=shape(1))
loss_latent<--0.5*tf$reduce_sum(1+z_log_sigma_sqtf$square(z_mean)-tf$exp(z_log_sigma_sq),reduction_indices=shape(1))
cost = tf$reduce_mean(loss_reconstruction + loss_latent)
return(cost)
}
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
network_weights<-model_init(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
networkOutput<-network_ParEval(x, network_weights, n_h)
cost=vae_optimizer(x, networkOutput)
optimizer = tf$train$AdamOptimizer(lr)$minimize(cost)
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
network_weights<-model_init(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
networkOutput<-network_ParEval(x, network_weights, n_h)
cost=vae_optimizer(x, networkOutput)
optimizer = tf$train$AdamOptimizer(lr)$minimize(cost)
function(n_inputs, n_outputs, constant=1){
low = -constant*sqrt(6.0/(n_inputs + n_outputs))
high = constant*sqrt(6.0/(n_inputs + n_outputs))
return(tf$random_uniform(shape(n_inputs, n_outputs), minval=low,
maxval=high, dtype=tf$float32))
}
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
network_weights<-model_init(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
networkOutput<-network_ParEval(x, network_weights, n_h)
cost=vae_optimizer(x, networkOutput)
optimizer = tf$train$AdamOptimizer(lr)$minimize(cost)
function(n_inputs, n_outputs, constant=1){
low = -constant*sqrt(6.0/(n_inputs + n_outputs))
high = constant*sqrt(6.0/(n_inputs + n_outputs))
return(tf$random_uniform(shape(n_inputs, n_outputs), minval=low,
maxval=high, dtype=tf$float32))
}
low = -constant*sqrt(6.0/(n_inputs + n_outputs))
img_size_flat=3072
x = tf$placeholder(tf$float32, shape=shape(NULL, img_size_flat),
name='x')
network_weights<-model_init(n.hidden.enc.1, n.hidden.enc.2,
n.hidden.dec.1, n.hidden.dec.2,
n_input, n_h)
